# -*- coding: utf-8 -*-
"""rag_chunk_token.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/151zPibs60KYMY3k8qkOvdTETY-TT7vN8
"""


from sentence_transformers import SentenceTransformer
import faiss
import pickle

file_path = "D:/codeNCKHSV/NCKHSV/benh_an.txt"

with open(file_path, "r", encoding="utf-8") as f:
    text = f.read()

docs = [doc.strip() for doc in text.split('---') if doc.strip()]

print(len(docs))
# print(docs)
for i in range(len(docs)):
  print(docs[i])

"""### CHUNK THEO TOKEN"""


from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer
import faiss
import pickle

# Dùng tokenizer tương thích với model embedding
tokenizer = AutoTokenizer.from_pretrained(
    "intfloat/multilingual-e5-base"
)
def chunk_text_by_tokens(
    text,
    tokenizer,
    chunk_size=300,
    overlap=50
):
    tokens = tokenizer.encode(text, add_special_tokens=False)

    chunks = []
    start = 0

    while start < len(tokens):
        end = start + chunk_size
        chunk_tokens = tokens[start:end]

        chunk_text = tokenizer.decode(
            chunk_tokens,
            skip_special_tokens=True
        )

        chunks.append(chunk_text)
        start = end - overlap

    return chunks
all_chunks = []
metadata = []

chunk_id = 0
documents = docs
for doc_id, doc_content in enumerate(documents): # Corrected loop to get doc_id and doc_content
    chunks = chunk_text_by_tokens(
        doc_content, # Pass the string directly
        tokenizer,
        chunk_size=300,
        overlap=50
    )

    for idx, chunk in enumerate(chunks):
        all_chunks.append(chunk)

        metadata.append({
            "chunk_id": chunk_id,
            "doc_id": doc_id, # Use the enumerated doc_id
            # "title": doc_content["title"], # Removed, as doc_content is a string
            "chunk_index": idx,
            "text": chunk,
            "token_length": len(
                tokenizer.encode(chunk, add_special_tokens=False)
            )
        })

        chunk_id += 1

print(len(all_chunks))
print(len(metadata))
print(all_chunks)
print(metadata)

embedder = SentenceTransformer("intfloat/multilingual-e5-base")

embeddings = embedder.encode(
    all_chunks,
    normalize_embeddings=True,
    show_progress_bar=True
).astype("float32")

dim = embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(embeddings)

faiss.write_index(index, "vector.index")

with open("metadata.pkl", "wb") as f:
    pickle.dump(metadata, f)

print("Đã lưu vector.index và metadata.pkl")

import os
print(os.getcwd())

index = faiss.read_index("vector.index")

with open("metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

embedder = SentenceTransformer("intfloat/multilingual-e5-base")

# @title
import faiss
import pickle
import pandas as pd

rows = []

for i in range(index.ntotal):
    vec = index.reconstruct(i)
    text = metadata[i].get("text", "") if isinstance(metadata[i], dict) else metadata[i]

    row = {
        "id": i,
        "text": text
    }

    # thêm từng chiều embedding vào cột
    for j, v in enumerate(vec):
        row[f"dim_{j}"] = float(v)

    rows.append(row)

df = pd.DataFrame(rows)

df

embeddings = [index.reconstruct(i) for i in range(index.ntotal)]
df_embed = pd.DataFrame(embeddings)

df_embed

def search(query, top_k=5):
    q_emb = embedder.encode(
        [query],
        normalize_embeddings=True
    ).astype("float32")

    scores, ids = index.search(q_emb, top_k)
    return [metadata[i] for i in ids[0]]

results = search("Kết quả cận lâm sàng ghi nhận nhịp tim chậm xoang (59 lần/phút)")
for r in results:
    print(
        f"[document {r['doc_id']} | chunk {r['chunk_index']} | tokens {r['token_length']}]"
    )
    print(r["text"])
    print("-" * 50)

from sentence_transformers import CrossEncoder

reranker = CrossEncoder(
    "cross-encoder/ms-marco-MiniLM-L-6-v2"
)

def search_with_rerank(query, faiss_top_k=20, final_top_k=5):
    # 1. Encode query
    q_emb = embedder.encode(
        [query],
        normalize_embeddings=True
    ).astype("float32")

    # 2. FAISS search
    scores, ids = index.search(q_emb, faiss_top_k)

    candidates = [
        metadata[i] for i in ids[0]
    ]

    # 3. Prepare pairs for cross-encoder
    pairs = [
        (query, c["text"]) for c in candidates
    ]

    # 4. Re-rank
    rerank_scores = reranker.predict(pairs)

    # 5. Sort by rerank score
    reranked = sorted(
        zip(candidates, rerank_scores),
        key=lambda x: x[1],
        reverse=True
    )

    # 6. Return top results
    return [
        {
            **item,
            "rerank_score": float(score)
        }
        for item, score in reranked[:final_top_k]
    ]

results = search_with_rerank(
    "không khó thở và không có dấu thần kinh khu trú",
    faiss_top_k=20,
    final_top_k=5
)

for r in results:
    print(r["rerank_score"], r["text"][:150])
    # print(
    #     f"[document {r['doc_id']} | chunk {r['chunk_index']} | tokens {r['token_length']}]"
    # )

results = search("tôi bị nhức đầu, ho ra máu, khó thở")
for r in results:
    print(
        f"[document {r['doc_id']} | chunk {r['chunk_index']} | tokens {r['token_length']}]"
    )
    print(r["text"])
    print("-" * 50)